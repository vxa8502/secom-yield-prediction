# Production Model Usage Guide

This guide shows how to load and use the trained model for production predictions.

## Quick Start

```python
import json
import joblib
import numpy as np
import pandas as pd

# Load trained model and threshold
model = joblib.load('models/production_model.pkl')

# Load optimal threshold from training results (auto-generated by pipeline)
with open('models/production_threshold.json') as f:
    threshold_info = json.load(f)
    OPTIMAL_THRESHOLD = threshold_info['optimal_threshold']
    FEATURE_SET = threshold_info['feature_set']

print(f"Loaded model with threshold={OPTIMAL_THRESHOLD:.3f}, features={FEATURE_SET}")

# Load appropriate preprocessor based on feature set
preprocessor = joblib.load(f'models/preprocessing_pipeline_{FEATURE_SET}.pkl')

# Example: Predict single wafer (8 LASSO-selected features)
# Listed in SHAP importance order (most to least important)
sensor_readings = {
    'feature_59': -1.3,    # Rank 1: Highest SHAP impact
    'feature_129': 3.1,    # Rank 2
    'feature_103': -0.5,   # Rank 3
    'feature_21': 2.5,     # Rank 4
    'feature_64': 0.8,     # Rank 5
    'feature_510': 0.2,    # Rank 6
    'feature_75': 1.2,     # Rank 7
    'feature_114': 0.1,    # Rank 8: Lowest SHAP impact
}

# Convert to DataFrame
X_new = pd.DataFrame([sensor_readings])

# Preprocess features
X_processed = preprocessor.transform(X_new)

# Get prediction probability
probability = model.predict_proba(X_processed)[0, 1]

# Apply optimal threshold
prediction = "FAIL" if probability >= OPTIMAL_THRESHOLD else "PASS"

print(f"Failure Probability: {probability:.3f}")
print(f"Prediction: {prediction}")
```

## Batch Predictions

```python
import json
import joblib
import numpy as np
import pandas as pd

# Load model and threshold
model = joblib.load('models/production_model.pkl')
with open('models/production_threshold.json') as f:
    config = json.load(f)
    OPTIMAL_THRESHOLD = config['optimal_threshold']
    preprocessor = joblib.load(f"models/preprocessing_pipeline_{config['feature_set']}.pkl")

# Load batch data (e.g., from manufacturing line)
batch_data = pd.read_csv('new_wafers.csv')

# Preprocess
X_batch = preprocessor.transform(batch_data)

# Predict
probabilities = model.predict_proba(X_batch)[:, 1]

# Apply threshold
predictions = ['FAIL' if p >= OPTIMAL_THRESHOLD else 'PASS' for p in probabilities]

# Create output report
results = pd.DataFrame({
    'wafer_id': batch_data.index,
    'failure_probability': probabilities,
    'prediction': predictions,
    'confidence': np.abs(probabilities - OPTIMAL_THRESHOLD)  # Distance from threshold
})

# Flag high-confidence failures for inspection
high_risk = results[
    (results['prediction'] == 'FAIL') &
    (results['failure_probability'] > 0.7)
]
print(f"High-risk wafers requiring inspection: {len(high_risk)}")
```

## Integration Example (REST API)

```python
import json
import joblib
import pandas as pd
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load model and configuration once at startup
MODEL = joblib.load('models/production_model.pkl')
with open('models/production_threshold.json') as f:
    CONFIG = json.load(f)
    THRESHOLD = CONFIG['optimal_threshold']
    PREPROCESSOR = joblib.load(f"models/preprocessing_pipeline_{CONFIG['feature_set']}.pkl")


@app.route('/predict', methods=['POST'])
def predict():
    """
    Expected JSON format (8 LASSO-selected features):
    {
        "features": {
            "feature_59": 2.5,
            "feature_129": -1.3,
            "feature_103": 0.8,
            "feature_21": 1.2,
            "feature_64": -0.5,
            "feature_510": 0.1,
            "feature_75": 3.1,
            "feature_114": 0.2
        }
    }
    """
    try:
        data = request.get_json()
        X = pd.DataFrame([data['features']])
        X_processed = PREPROCESSOR.transform(X)

        probability = MODEL.predict_proba(X_processed)[0, 1]
        prediction = "FAIL" if probability >= THRESHOLD else "PASS"

        return jsonify({
            'prediction': prediction,
            'failure_probability': float(probability),
            'threshold': THRESHOLD,
            'model': CONFIG.get('model_name', 'unknown'),
            'status': 'success'
        })

    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 400


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## Model Artifacts

After running the modeling pipeline, the following files are generated:

| File | Description | Size |
|------|-------------|------|
| `models/production_model.pkl` | Trained SVM classifier with ADASYN | ~193 KB |
| `models/production_threshold.json` | Optimal threshold configuration | <1 KB |
| `models/preprocessing_pipeline_lasso.pkl` | LASSO (8 features) preprocessor | ~18 KB |
| `models/preprocessing_pipeline_pca.pkl` | PCA preprocessor | ~356 KB |
| `models/preprocessing_pipeline_basic.pkl` | Basic preprocessor | ~11 KB |

## Deployment Checklist

- [ ] Run full pipeline: `make pipeline`
- [ ] Verify model artifacts exist in `models/`
- [ ] Verify `models/production_threshold.json` contains correct threshold (auto-generated)
- [ ] Test prediction on sample data
- [ ] Benchmark inference latency (see `reports/latency_benchmark.csv`)
- [ ] Set up monitoring for data drift (see `reports/figures/`)
- [ ] Configure alerting for predictions near threshold (borderline cases)

## Performance Expectations

Based on production model evaluation:

- **Latency:** 0.10ms per prediction (single wafer)
- **Throughput:** ~9,800 predictions/second (batch mode)
- **Sensitivity:** 66.7% (catches 2/3 of actual failures)
- **Specificity:** 78.2% (correctly identifies passing wafers)
- **G-Mean:** 72.2% (test set; CV G-Mean: 71.0%)
- **AUC-ROC:** 78.3%
- **Memory:** ~10 MB (model + preprocessor loaded in RAM)

## Troubleshooting

### Missing features error
```python
# Ensure all required features are present (8 for LASSO)
expected_features = preprocessor.feature_names_in_
missing = set(expected_features) - set(X_new.columns)
print(f"Missing features: {missing}")
```

### Poor calibration warning
```python
# If probabilities are poorly calibrated, use CalibratedClassifierCV
from sklearn.calibration import CalibratedClassifierCV

# Train calibrated model (one-time, save as new artifact)
calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=5)
calibrated_model.fit(X_train, y_train)
joblib.dump(calibrated_model, 'models/production_model_calibrated.pkl')
```

### Threshold tuning for new business requirements
```python
# If cost of false negatives changes, retune threshold
from src.threshold import find_optimal_threshold_cv
import json

# Example: Prioritize sensitivity (catch more failures, accept more false alarms)
result = find_optimal_threshold_cv(
    pipeline=model,
    X_train=X_train,
    y_train=y_train,
    threshold_range=(0.3, 0.7),  # Lower range = higher sensitivity
)
print(f"New optimal threshold: {result['optimal_threshold']:.3f}")
print(f"G-Mean at new threshold: {result['cv_gmean']:.4f}")

# Save updated threshold
with open('models/production_threshold.json', 'r+') as f:
    config = json.load(f)
    config['optimal_threshold'] = result['optimal_threshold']
    f.seek(0)
    json.dump(config, f, indent=2)
    f.truncate()
```

## Assumptions & Limitations

### Dataset Constraints
- **Sample Size:** 1,567 wafers is relatively small for deep learning approaches; tree-based and linear models are more appropriate
- **Class Imbalance:** 6.6% failure rate (14:1 ratio) requires careful handling; accuracy metrics are misleading
- **Anonymized Features:** Original sensor names unavailable, limiting domain-specific feature engineering
- **Snapshot Data:** Single measurement per wafer; no temporal sequences within wafer processing

### Methodological Choices
- **Train/Test Split:** Simple stratified 80/20 split used; time-series cross-validation not applied (timestamps span only 30 days with batch processing)
- **Feature Selection:** LASSO selects 8 features; may miss weak but collectively useful signals
- **Threshold Tuning:** Optimized for G-Mean (balanced sensitivity/specificity); different business costs may require custom objective
- **No Calibration:** Probability outputs not post-calibrated; use CalibratedClassifierCV if precise probabilities required

### Deployment Considerations
- **Data Drift:** Model assumes test distribution matches training; monitor feature distributions in production
- **Missing Values:** Model expects preprocessed inputs; raw sensor data requires imputation pipeline
- **Latency:** Sub-millisecond inference; suitable for real-time but not optimized for edge deployment
- **Interpretability:** SHAP explanations available but computationally expensive for KernelExplainer (SVM)

### Known Gaps vs. Production Systems
- No online learning / incremental retraining
- No A/B testing infrastructure
- No automated drift detection alerts
- No integration with manufacturing execution system (MES)

---

## Next Steps

1. **Monitor in production:** Track prediction distribution drift
2. **Retrain schedule:** Quarterly retraining with new manufacturing data
3. **A/B testing:** Compare production model vs. challenger models
4. **Feature engineering:** Incorporate new sensor data as available
5. **Explainability:** Use SHAP for individual wafer failure root cause analysis
