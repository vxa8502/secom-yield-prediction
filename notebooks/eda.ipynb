{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SECOM Semiconductor Defect Prediction - Exploratory Data Analysis\n",
    "Author: Victoria A.\n",
    "Project: Yield forecasting with UCI SECOM dataset\n",
    "\n",
    "This script performs initial exploratory data analysis on the SECOM dataset,\n",
    "including missing data analysis, correlation exploration, and feature selection.\n",
    "\n",
    "When run as script (make eda): generates figures + markdown report silently\n",
    "When run interactively: shows plots and prints analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import io\n",
    "# Suppress plotting-related warnings (missingno, matplotlib, etc.)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33669b88",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import missingno as msno\n",
    "\n",
    "# Quiet mode: suppress terminal output when running as script\n",
    "# Interactive mode (notebook/ipython): show output\n",
    "INTERACTIVE = 'ipykernel' in sys.modules or hasattr(sys, 'ps1')\n",
    "\n",
    "def log(msg):\n",
    "    \"\"\"Print only in interactive mode.\"\"\"\n",
    "    if INTERACTIVE:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42012e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup project root path for imports and data loading\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d48c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data directory paths\n",
    "RAW_DATA_DIR = project_root / 'data' / 'raw' / 'secom'\n",
    "REPORTS_DIR = project_root / 'reports'\n",
    "FIGURES_DIR = REPORTS_DIR / 'figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc3043",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualization configuration - consistent across all plots\n",
    "# High DPI (300) for publication-quality figures in markdown report\n",
    "VIZ_CONFIG = {\n",
    "    'dpi': 300,\n",
    "    'style': 'seaborn-v0_8-darkgrid',\n",
    "    'palette': 'colorblind',\n",
    "    'context': 'notebook',\n",
    "\n",
    "    # Font sizes\n",
    "    'title_fontsize': 12,\n",
    "    'label_fontsize': 10,\n",
    "    'tick_fontsize': 9,\n",
    "\n",
    "    # Brand color palette\n",
    "    'primary': '#1428A0',\n",
    "    'secondary': '#2596be',\n",
    "    'pass_color': '#00C1B0',\n",
    "    'fail_color': '#FFB246',\n",
    "    'neutral': '#757575',\n",
    "\n",
    "    # Heatmap colormap (brand blue gradient)\n",
    "    'heatmap_cmap': 'Blues'\n",
    "}\n",
    "\n",
    "# Track generated figures for markdown report\n",
    "GENERATED_FIGURES = []\n",
    "\n",
    "\n",
    "def save_figure(fig, filename, caption=\"\"):\n",
    "    \"\"\"Save figure and track for markdown report generation.\"\"\"\n",
    "    filepath = FIGURES_DIR / filename\n",
    "    fig.savefig(filepath, dpi=VIZ_CONFIG['dpi'], bbox_inches='tight', facecolor='white')\n",
    "    GENERATED_FIGURES.append({'filename': filename, 'caption': caption})\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86f6ca",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features (590 sensor readings)\n",
    "X = pd.read_csv(RAW_DATA_DIR / 'secom.data', sep=' ', header=None, index_col=False)\n",
    "\n",
    "# Load labels (binary classification: originally -1 = pass, 1 = fail)\n",
    "y = pd.read_csv(RAW_DATA_DIR / 'secom_labels.data', sep=' ', header=None,\n",
    "                names=['target', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b351a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset dimensions\n",
    "log(f\"Dataset Dimensions:\")\n",
    "log(f\"  Features (X): {X.shape[0]:,} samples × {X.shape[1]} features\")\n",
    "log(f\"  Labels (y):   {y.shape[0]:,} samples × {y.shape[1]} columns\")\n",
    "log(f\"\\nFirst 3 samples (showing first 5 features):\")\n",
    "log(X.head(3).iloc[:, :5].to_string())\n",
    "log(f\"\\nFirst 3 labels:\")\n",
    "log(y.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels for analysis\n",
    "column_names = [f'feature_{i}' for i in range(1, X.shape[1] + 1)]\n",
    "X.columns = column_names\n",
    "\n",
    "# Convert to sklearn standard: 0 = Pass (majority), 1 = Fail (minority)\n",
    "y['target'] = y['target'].map({-1: 0, 1: 1})\n",
    "\n",
    "# Parse timestamp\n",
    "y['timestamp'] = pd.to_datetime(y['timestamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "log(f\"Data Preparation Summary:\")\n",
    "log(f\"  Feature columns: feature_1 through feature_{X.shape[1]}\")\n",
    "log(f\"  Target encoding: 0 = Pass (majority), 1 = Fail (minority)\")\n",
    "log(f\"  Target unique values: {sorted(y['target'].unique())}\")\n",
    "log(f\"  Date range: {y['timestamp'].min().date()} to {y['timestamp'].max().date()}\")\n",
    "log(f\"  Duration: {(y['timestamp'].max() - y['timestamp'].min()).days} days\")\n",
    "log(f\"\\nData types:\")\n",
    "log(f\"  Features: {X.dtypes.value_counts().to_dict()}\")\n",
    "log(f\"  Labels: target={y['target'].dtype}, timestamp={y['timestamp'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics for all features\n",
    "desc_stats = X.describe()\n",
    "desc_stats.T.to_csv(REPORTS_DIR / 'feature_statistics.csv')\n",
    "\n",
    "minimum_values = X.min().sort_values(ascending=True)\n",
    "maximum_values = X.max().sort_values(ascending=True)\n",
    "mean_values = X.mean().sort_values(ascending=True)\n",
    "std_values = X.std().sort_values(ascending=True)\n",
    "median_values = X.median().sort_values(ascending=True)\n",
    "\n",
    "log(f\"\\nBASIC STATISTICS SUMMARY\")\n",
    "log(f\"Aggregate statistics across all {X.shape[1]} features:\")\n",
    "log(f\"  Min value:   {minimum_values.min():>12.4f}  (feature: {minimum_values.index[0]})\")\n",
    "log(f\"  Max value:   {maximum_values.max():>12.4f}  (feature: {maximum_values.index[-1]})\")\n",
    "log(f\"  Mean (avg):  {mean_values.mean():>12.4f}\")\n",
    "log(f\"  Std (avg):   {std_values.mean():>12.4f}\")\n",
    "log(f\"  Median (med):{median_values.median():>12.4f}\")\n",
    "log(f\"\\nDetailed statistics saved: {REPORTS_DIR / 'feature_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance analysis - identify low/high variance features\n",
    "cv_values = std_values / mean_values.abs().replace(0, np.nan)\n",
    "exact_zero_variance = std_values[std_values == 0]\n",
    "near_zero_variance = cv_values[cv_values < 0.01]\n",
    "high_variance = std_values[std_values > std_values.quantile(0.95)]\n",
    "\n",
    "log(f\"\\nVARIANCE ANALYSIS\")\n",
    "log(f\"Features with zero variance (constant):           {len(exact_zero_variance):>4}\")\n",
    "if len(exact_zero_variance) > 0:\n",
    "    log(f\"  Examples: {', '.join(exact_zero_variance.index[:3].tolist())}\")\n",
    "\n",
    "log(f\"Features with near-zero variance (CV < 0.01):     {len(near_zero_variance):>4}\")\n",
    "if len(near_zero_variance) > 0:\n",
    "    log(f\"  Examples: {', '.join(near_zero_variance.index[:3].tolist())}\")\n",
    "\n",
    "log(f\"Features with high variance (>95th percentile):   {len(high_variance):>4}\")\n",
    "if len(high_variance) > 0:\n",
    "    log(f\"  Examples: {', '.join(high_variance.index[:3].tolist())}\")\n",
    "    log(f\"  Threshold (95th percentile): {std_values.quantile(0.95):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb216360",
   "metadata": {},
   "source": [
    "### DUPLICATE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate samples\n",
    "duplicate_features = X.duplicated().sum()\n",
    "combined = pd.concat([X, y['target']], axis=1)\n",
    "duplicate_combined = combined.duplicated().sum()\n",
    "duplicate_timestamps = y['timestamp'].duplicated().sum()\n",
    "\n",
    "log(f\"\\nDUPLICATE DETECTION\")\n",
    "log(f\"Duplicate rows (features only):              {duplicate_features:>4}\")\n",
    "log(f\"Duplicate rows (features + target):          {duplicate_combined:>4}\")\n",
    "log(f\"Duplicate timestamps:                        {duplicate_timestamps:>4}\")\n",
    "\n",
    "if duplicate_timestamps > 0:\n",
    "    pct = (duplicate_timestamps / len(y)) * 100\n",
    "    log(f\"\\nInterpretation: {pct:.2f}% of samples share timestamps\")\n",
    "    log(f\"  Likely cause: Multiple wafers processed simultaneously\")\n",
    "else:\n",
    "    log(f\"\\nInterpretation: All timestamps are unique (no batch processing detected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f57be4",
   "metadata": {},
   "source": [
    "### CLASS IMBALANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42214340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "class_balance = y['target'].value_counts().sort_index()\n",
    "passed_wafers = class_balance[0]\n",
    "failed_wafers = class_balance[1]\n",
    "failed_pct = (failed_wafers / class_balance.sum()) * 100\n",
    "imbalance_ratio = passed_wafers / failed_wafers\n",
    "naive_accuracy = max(passed_wafers, failed_wafers) / class_balance.sum() * 100\n",
    "\n",
    "log(f\"\\nCLASS IMBALANCE ANALYSIS\")\n",
    "log(f\"Class distribution:\")\n",
    "log(f\"  0 = Pass: {passed_wafers:>5} wafers ({100 - failed_pct:>5.2f}%)\")\n",
    "log(f\"  1 = Fail: {failed_wafers:>5} wafers ({failed_pct:>5.2f}%)\")\n",
    "log(f\"\\nImbalance ratio:        {imbalance_ratio:.1f}:1 (Pass:Fail)\")\n",
    "log(f\"Naive accuracy baseline: {naive_accuracy:.2f}% (always predict majority class)\")\n",
    "log(f\"\\nIMPLICATIONS:\")\n",
    "log(f\"  - SEVERE imbalance detected ({imbalance_ratio:.0f}:1)\")\n",
    "log(f\"  - Must use balanced metrics: G-Mean, F1, AUC-ROC (NOT accuracy)\")\n",
    "log(f\"  - Requires resampling (SMOTE/ADASYN) or class weighting\")\n",
    "log(f\"  - Confusion matrix more informative than accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal class imbalance analysis\n",
    "y_sorted = y.sort_values('timestamp')\n",
    "time_grouped = y_sorted.groupby([pd.Grouper(key='timestamp', freq='D'), 'target']).size().unstack(fill_value=0)\n",
    "time_grouped.columns = ['Pass', 'Fail']\n",
    "time_grouped['Total'] = time_grouped['Pass'] + time_grouped['Fail']\n",
    "time_grouped['Fail_Rate'] = (time_grouped['Fail'] / time_grouped['Total']) * 100\n",
    "time_grouped['Imbalance_Ratio'] = time_grouped['Pass'] / time_grouped['Fail'].replace(0, 1)\n",
    "\n",
    "log(f\"\\nTemporal Analysis ({len(time_grouped)} days of production):\")\n",
    "log(f\"  Failure rate range:   {time_grouped['Fail_Rate'].min():.2f}% to {time_grouped['Fail_Rate'].max():.2f}%\")\n",
    "log(f\"  Failure rate std dev: {time_grouped['Fail_Rate'].std():.2f}%\")\n",
    "log(f\"  Days with 0 failures: {(time_grouped['Fail'] == 0).sum()}\")\n",
    "log(f\"  Days with 100% fails: {(time_grouped['Pass'] == 0).sum()}\")\n",
    "log(f\"  Days with <5 samples: {(time_grouped['Total'] < 5).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(VIZ_CONFIG['style'])\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 5), dpi=VIZ_CONFIG['dpi'], sharex=True)\n",
    "\n",
    "# Plot 1: Failure Rate (%)\n",
    "axes[0].plot(time_grouped.index, time_grouped['Fail_Rate'], \n",
    "             color=VIZ_CONFIG['fail_color'], linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
    "axes[0].axhline(y=time_grouped['Fail_Rate'].mean(), color=VIZ_CONFIG['fail_color'], linestyle='--',\n",
    "                linewidth=2, label=f'Mean: {time_grouped[\"Fail_Rate\"].mean():.2f}%', alpha=0.7)\n",
    "axes[0].fill_between(time_grouped.index, 0, time_grouped['Fail_Rate'], \n",
    "                      color=VIZ_CONFIG['fail_color'], alpha=0.2)\n",
    "axes[0].set_ylabel('Failure Rate (%)', fontweight='bold')\n",
    "axes[0].set_title('Class Imbalance Analysis Over Time', fontweight='bold', fontsize=14)\n",
    "axes[0].legend(loc='best', framealpha=0.9)\n",
    "axes[0].grid(True, alpha=0.1)\n",
    "\n",
    "# Plot 2: Sample Volume (to show if low counts affect reliability)\n",
    "axes[1].bar(time_grouped.index, time_grouped['Pass'], \n",
    "            color=VIZ_CONFIG['pass_color'], alpha=0.6, label='Pass', width=0.8)\n",
    "axes[1].bar(time_grouped.index, time_grouped['Fail'], \n",
    "            bottom=time_grouped['Pass'], color=VIZ_CONFIG['fail_color'], \n",
    "            alpha=0.6, label='Fail', width=0.8)\n",
    "axes[1].set_ylabel('Sample Count', fontweight='bold')\n",
    "axes[1].set_xlabel('Date', fontweight='bold')\n",
    "axes[1].legend(loc='best', framealpha=0.9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Format x-axis\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'class_imbalance_over_time.png', 'Failure rate and sample volume over time')\n",
    "if INTERACTIVE: plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ffe5b",
   "metadata": {},
   "source": [
    "### MISSING DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data patterns\n",
    "missing_by_feature = X.isna().sum()\n",
    "total_missing = missing_by_feature.sum()\n",
    "total_cells = X.shape[0] * X.shape[1]\n",
    "missing_pct_overall = (total_missing / total_cells) * 100\n",
    "\n",
    "missing_counts = missing_by_feature[missing_by_feature > 0].sort_values(ascending=False)\n",
    "missing_pct_by_feature = (missing_counts / X.shape[0]) * 100\n",
    "\n",
    "complete_features = (missing_by_feature == 0).sum()\n",
    "half_missing = (missing_pct_by_feature >= 50).sum()\n",
    "mostly_missing = (missing_pct_by_feature >= 90).sum()\n",
    "features_half_missing = missing_pct_by_feature[missing_pct_by_feature >= 50].sort_values(ascending=False)\n",
    "\n",
    "log(f\"\\nMISSING DATA ANALYSIS\")\n",
    "log(f\"Overall statistics:\")\n",
    "log(f\"  Total missing values: {total_missing:,} / {total_cells:,} ({missing_pct_overall:.2f}%)\")\n",
    "log(f\"\\nFeature-level statistics:\")\n",
    "log(f\"  Complete features (0% missing):   {complete_features:>4}\")\n",
    "log(f\"  Features with missing data:       {len(missing_counts):>4}\")\n",
    "log(f\"  Features >50% missing:            {half_missing:>4}\")\n",
    "log(f\"  Features >90% missing:            {mostly_missing:>4}\")\n",
    "log(f\"\\nTop 10 features with highest missingness (>{50}%):\")\n",
    "for i, (feat, pct) in enumerate(features_half_missing.head(10).items(), 1):\n",
    "    count = missing_counts[feat]\n",
    "    log(f\"  {i:>2}. {feat:<15} {count:>5} / {X.shape[0]} ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if missingness correlates with target (MCAR vs MAR/MNAR)\n",
    "missing_indicators = X.isna().astype(int)\n",
    "missing_indicators.columns = [f'{col}_missing' for col in X.columns]\n",
    "\n",
    "missing_target_corr = []\n",
    "for feature in missing_counts.index:\n",
    "    missing_col = f'{feature}_missing'\n",
    "    corr = missing_indicators[missing_col].corr(y['target'])\n",
    "    missing_target_corr.append({\n",
    "        'Feature': feature,\n",
    "        'Correlation': corr,\n",
    "        'Abs_Correlation': abs(corr),\n",
    "        'Missing_Pct': missing_pct_by_feature[feature]\n",
    "    })\n",
    "\n",
    "missing_corr_df = pd.DataFrame(missing_target_corr).sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "significant_threshold = 0.1\n",
    "significant_missing = missing_corr_df[missing_corr_df['Abs_Correlation'] > significant_threshold]\n",
    "\n",
    "log(f\"\\nMissingness-Target Correlation (testing for MAR/MNAR):\")\n",
    "log(f\"  Features with |r| > {significant_threshold}: {len(significant_missing)}\")\n",
    "if len(significant_missing) > 0:\n",
    "    log(f\"  Interpretation: Non-random missingness detected (MAR/MNAR)\")\n",
    "    log(f\"\\n  Top 5 features with strongest missingness-target correlation:\")\n",
    "    for i, row in missing_corr_df.head(5).iterrows():\n",
    "        log(f\"    {row['Feature']:<20} r={row['Correlation']:>7.4f}  ({row['Missing_Pct']:.1f}% missing)\")\n",
    "else:\n",
    "    log(f\"  Interpretation: Missingness appears random (MCAR)\")\n",
    "log(f\"\\nIMPLICATION: {'Missingness pattern may be informative for prediction' if len(significant_missing) > 0 else 'Missing values can be safely imputed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missingness patterns\n",
    "X_viz = X[features_half_missing.index]\n",
    "log(f\"\\nVisualizing missingness patterns ({len(features_half_missing)} features with >50% missing):\")\n",
    "fig = msno.matrix(X_viz).get_figure()\n",
    "save_figure(fig, 'missing_data_matrix.png', 'Missing data matrix for features with >50% missing values')\n",
    "if INTERACTIVE: plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig = msno.heatmap(X_viz).get_figure()\n",
    "save_figure(fig, 'missing_data_heatmap.png', 'Nullity correlation heatmap showing co-occurrence of missing values')\n",
    "if INTERACTIVE: plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze nullity correlation (missingness co-occurrence)\n",
    "nullity_matrix = X.notna().astype(int)\n",
    "nullity_corr = nullity_matrix.corr()\n",
    "nullity_corr.to_csv(REPORTS_DIR / 'nullity_correlation_matrix.csv')\n",
    "\n",
    "upper_triangle = nullity_corr.where(np.triu(np.ones(nullity_corr.shape), k=1).astype(bool))\n",
    "correlations_stacked = upper_triangle.stack().sort_values(key=abs, ascending=False)\n",
    "perfect_corr = correlations_stacked[correlations_stacked.abs() >= 1.0]\n",
    "\n",
    "log(f\"\\nNullity Correlation Analysis:\")\n",
    "log(f\"  Feature pairs with perfect missingness correlation: {len(perfect_corr)}\")\n",
    "log(f\"  Saved full nullity correlation matrix to: nullity_correlation_matrix.csv\")\n",
    "if len(perfect_corr) > 0:\n",
    "    log(f\"  Interpretation: {len(perfect_corr)} feature pairs always missing together (sensor groups?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5aa279",
   "metadata": {},
   "source": [
    "### CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature-target correlations\n",
    "joined = pd.concat([X, y['target']], axis=1)\n",
    "corr_matrix = joined.corr().abs()\n",
    "corr_matrix.to_csv(REPORTS_DIR / 'full_correlation_matrix.csv')\n",
    "\n",
    "target_corr = corr_matrix['target'].drop('target').sort_values(ascending=False)\n",
    "top_corr_features = corr_matrix['target'].sort_values(ascending=False).head(20)\n",
    "top_corr_features_list = top_corr_features.index\n",
    "\n",
    "log(f\"\\nCORRELATION ANALYSIS\")\n",
    "log(f\"Feature-target correlation summary:\")\n",
    "log(f\"  Max correlation:      {target_corr.max():>7.4f} (feature: {target_corr.idxmax()})\")\n",
    "log(f\"  Mean correlation:     {target_corr.mean():>7.4f}\")\n",
    "log(f\"  Median correlation:   {target_corr.median():>7.4f}\")\n",
    "log(f\"  Features with |r| > 0.1: {(target_corr > 0.1).sum():>4}\")\n",
    "log(f\"  Features with |r| > 0.2: {(target_corr > 0.2).sum():>4}\")\n",
    "log(f\"\\nTop 10 features most correlated with target:\")\n",
    "for i, (feat, corr_val) in enumerate(target_corr.head(10).items(), 1):\n",
    "    log(f\"  {i:>2}. {feat:<20} {corr_val:>7.4f}\")\n",
    "log(f\"\\nSaved full {corr_matrix.shape[0]}×{corr_matrix.shape[1]} correlation matrix to: full_correlation_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec90089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap (top 20 features only)\n",
    "top_corr_matrix = corr_matrix.loc[top_corr_features_list, top_corr_features_list]\n",
    "fig, ax = plt.subplots(figsize=(10, 8), dpi=VIZ_CONFIG['dpi'])\n",
    "plt.style.use(VIZ_CONFIG['style'])\n",
    "sns.heatmap(top_corr_matrix, annot=True, fmt=\".2f\", cmap=VIZ_CONFIG['heatmap_cmap'],\n",
    "            cbar_kws={'label': 'Absolute Correlation'}, vmin=0, vmax=1, ax=ax)\n",
    "ax.set_title(f'Correlation Heatmap: Top {len(top_corr_features_list)-1} Features + Target',\n",
    "             fontsize=VIZ_CONFIG['title_fontsize'], fontweight='bold')\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'correlation_heatmap_top20.png', 'Correlation heatmap of top 20 features most correlated with target')\n",
    "if INTERACTIVE: plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18387840",
   "metadata": {},
   "source": [
    "### DISTRIBUTION ANALYSIS: Top Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distributions of top features (excluding target)\n",
    "top_features_list = [f for f in top_corr_features_list if f != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e197ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical testing for distribution differences (Mann-Whitney U test)\n",
    "top_6_features = top_features_list[:6]\n",
    "log(f\"Statistical Tests: Distribution Differences Between Pass and Fail\\n\")\n",
    "log(f\"{'Feature':<20} {'Correlation':<12} {'Mann-Whitney U p-value':<25} {'Significant?':<15}\")\n",
    "log(\"-\" * 80)\n",
    "\n",
    "for feature in top_6_features:\n",
    "    pass_data = X.loc[y['target'] == 0, feature].dropna()\n",
    "    fail_data = X.loc[y['target'] == 1, feature].dropna()\n",
    "\n",
    "    if len(pass_data) > 0 and len(fail_data) > 0:\n",
    "        statistic, p_value = mannwhitneyu(pass_data, fail_data, alternative='two-sided')\n",
    "        significant = \"YES\" if p_value < 0.05 else \"NO\"\n",
    "        corr_val = corr_matrix.loc[feature, 'target']\n",
    "        log(f\"{feature:<20} {corr_val:<12.4f} {p_value:<25.6e} {significant:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions for top 6 features (most correlated with target)\n",
    "log(f\"\\nVisualizing distributions for top 6 features by target correlation\")\n",
    "\n",
    "plt.style.use(VIZ_CONFIG['style'])\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 9), dpi=VIZ_CONFIG['dpi'])\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_6_features):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Split by target class\n",
    "    pass_data = X.loc[y['target'] == 0, feature].dropna()\n",
    "    fail_data = X.loc[y['target'] == 1, feature].dropna()\n",
    "\n",
    "    # Plot overlapping histograms\n",
    "    ax.hist(pass_data, bins=30, alpha=0.6, label='Pass', color=VIZ_CONFIG['pass_color'], edgecolor='black')\n",
    "    ax.hist(fail_data, bins=30, alpha=0.6, label='Fail', color=VIZ_CONFIG['fail_color'], edgecolor='black')\n",
    "\n",
    "    ax.set_title(f'{feature}\\n(r = {corr_matrix.loc[feature, \"target\"]:.3f})',\n",
    "                 fontsize=VIZ_CONFIG['title_fontsize'], fontweight='bold')\n",
    "    ax.set_xlabel('Value', fontsize=VIZ_CONFIG['label_fontsize'])\n",
    "    ax.set_ylabel('Frequency', fontsize=VIZ_CONFIG['label_fontsize'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribution of Top 6 Features by Target Class',\n",
    "             fontsize=VIZ_CONFIG['title_fontsize'], fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'top_features_distributions.png', 'Distribution comparison of top 6 features by Pass/Fail class')\n",
    "if INTERACTIVE: plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9297e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Boxplots for top 6 features (shows outliers and spread)\n",
    "plt.style.use(VIZ_CONFIG['style'])\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 8), dpi=VIZ_CONFIG['dpi'])\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_6_features):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Create boxplot data\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Value': X[feature],\n",
    "        'Target': y['target'].map({0: 'Pass', 1: 'Fail'})\n",
    "    }).dropna()\n",
    "\n",
    "    # Boxplot by target class\n",
    "    plot_data.boxplot(column='Value', by='Target', ax=ax,\n",
    "                      patch_artist=True,\n",
    "                      boxprops=dict(facecolor=VIZ_CONFIG['secondary'], color='black'),\n",
    "                      medianprops=dict(color=VIZ_CONFIG['primary'], linewidth=2),\n",
    "                      whiskerprops=dict(color='black'),\n",
    "                      capprops=dict(color='black'))\n",
    "\n",
    "    ax.set_title(f'{feature} (r = {corr_matrix.loc[feature, \"target\"]:.3f})',\n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Target Class')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.get_figure().suptitle('')  # Remove auto-generated title\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Boxplots: Top 6 Features by Target Class (outliers visible)',\n",
    "             fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'top_features_boxplots.png', 'Boxplots showing outlier distribution in top 6 features')\n",
    "if INTERACTIVE: plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e3b23",
   "metadata": {},
   "source": [
    "### OUTLIER DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee18d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define helper function for IQR-based outlier detection\n",
    "def count_outliers_iqr(series):\n",
    "    \"\"\"Count outliers using IQR method (values beyond 1.5*IQR from Q1/Q3)\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (series < lower_bound) | (series > upper_bound)\n",
    "    return outliers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25054e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate outliers per feature\n",
    "outlier_counts = X.apply(count_outliers_iqr)\n",
    "outlier_pct = (outlier_counts / len(X)) * 100\n",
    "features_with_outliers = outlier_counts[outlier_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display outlier summary statistics\n",
    "log(f\"Outlier summary (IQR method: beyond Q1-1.5*IQR or Q3+1.5*IQR):\")\n",
    "log(f\"  Features with at least one outlier: {len(features_with_outliers)}\")\n",
    "log(f\"  Total outlier values across all features: {outlier_counts.sum():,} / {(X.shape[0] * X.shape[1]):,} ({(outlier_counts.sum() / (X.shape[0] * X.shape[1])) * 100:.2f}%)\")\n",
    "log(f\"  Mean outliers per feature: {outlier_counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 features with most outliers\n",
    "log(f\"Top 10 features with most outliers:\")\n",
    "top_outlier_features = features_with_outliers.head(10)\n",
    "outlier_summary = pd.DataFrame({\n",
    "    'Feature': top_outlier_features.index,\n",
    "    'Outlier Count': top_outlier_features.values,\n",
    "    'Outlier %': outlier_pct[top_outlier_features.index].values\n",
    "})\n",
    "log(outlier_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count outliers per sample (vectorized)\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bounds = Q1 - 1.5 * IQR\n",
    "upper_bounds = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca71e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean mask: True where value is outlier\n",
    "is_outlier = (X < lower_bounds) | (X > upper_bounds)\n",
    "\n",
    "# Count outliers per sample (sum across columns)\n",
    "outliers_per_sample = is_outlier.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze extreme samples\n",
    "extreme_samples = outliers_per_sample[outliers_per_sample > 45].sort_values(ascending=False)\n",
    "\n",
    "log(f\"Samples (wafers) with outliers in multiple features:\")\n",
    "log(f\"  Total samples that are outliers in >45 features: {len(extreme_samples)}\")\n",
    "\n",
    "most_extreme_idx = extreme_samples.idxmax()\n",
    "most_extreme_count = extreme_samples.max()\n",
    "log(f\"  Most extreme: Sample #{most_extreme_idx} is outlier in {most_extreme_count} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if extreme outlier samples correlate with failures\n",
    "log(f\"Do extreme outlier samples tend to fail more?\")\n",
    "fail_rate_extreme = (y.loc[extreme_samples.index, 'target'] == 1).sum() / len(extreme_samples) * 100\n",
    "fail_rate_normal = (y.loc[~y.index.isin(extreme_samples.index), 'target'] == 1).sum() / (len(y) - len(extreme_samples)) * 100\n",
    "overall_fail_rate = (y['target'] == 1).sum() / len(y) * 100\n",
    "\n",
    "log(f\"  Extreme outliers (>45 outlier features): {fail_rate_extreme:.1f}% failure rate ({len(extreme_samples)} samples)\")\n",
    "log(f\"  Normal samples: {fail_rate_normal:.1f}% failure rate ({len(y) - len(extreme_samples)} samples)\")\n",
    "log(f\"  Overall failure rate: {overall_fail_rate:.1f}%\")\n",
    "\n",
    "if fail_rate_extreme > overall_fail_rate * 1.5:\n",
    "    log(f\"KEY INSIGHT: Extreme outliers have {fail_rate_extreme/overall_fail_rate:.1f}x higher failure rate!\")\n",
    "elif fail_rate_extreme < overall_fail_rate * 0.8:\n",
    "    log(f\"KEY INSIGHT: Extreme outliers have LOWER failure rate than average\")\n",
    "else:\n",
    "    log(f\"Extreme outliers have similar failure rate to overall population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb296b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: Do outliers correlate with failures? (Chi-Square)\n",
    "log(f\"\\nStatistical Test: Outlier-Failure Association (Chi-Square)\\n\")\n",
    "log(f\"Testing features where outliers have >5x failure rate...\\n\")\n",
    "\n",
    "extreme_features = []\n",
    "chi_square_results = []\n",
    "\n",
    "for feature in features_with_outliers.index:\n",
    "    feature_is_outlier = is_outlier[feature]\n",
    "\n",
    "    if feature_is_outlier.sum() > 0:\n",
    "        fail_rate_outliers = (y.loc[feature_is_outlier, 'target'] == 1).sum() / feature_is_outlier.sum() * 100\n",
    "        fail_rate_normal = (y.loc[~feature_is_outlier, 'target'] == 1).sum() / (~feature_is_outlier).sum() * 100\n",
    "\n",
    "        if fail_rate_outliers > fail_rate_normal * 5.0:\n",
    "            # Contingency table: [outlier_pass, outlier_fail], [normal_pass, normal_fail]\n",
    "            outlier_pass = (y.loc[feature_is_outlier, 'target'] == 0).sum()\n",
    "            outlier_fail = (y.loc[feature_is_outlier, 'target'] == 1).sum()\n",
    "            normal_pass = (y.loc[~feature_is_outlier, 'target'] == 0).sum()\n",
    "            normal_fail = (y.loc[~feature_is_outlier, 'target'] == 1).sum()\n",
    "\n",
    "            contingency_table = [[outlier_pass, outlier_fail], [normal_pass, normal_fail]]\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "            log(f\"{feature}:\")\n",
    "            log(f\"  Outliers: {feature_is_outlier.sum()} samples, {fail_rate_outliers:.1f}% failure rate\")\n",
    "            log(f\"  Normal: {(~feature_is_outlier).sum()} samples, {fail_rate_normal:.1f}% failure rate\")\n",
    "            log(f\"  Risk Ratio: {fail_rate_outliers/fail_rate_normal:.1f}x\")\n",
    "            log(f\"  Chi-Square: {chi2:.2f}, p-value: {p_value:.6e} {'(SIGNIFICANT)' if p_value < 0.05 else '(not significant)'}\\n\")\n",
    "\n",
    "            extreme_features.append(feature)\n",
    "            chi_square_results.append({\n",
    "                'Feature': feature,\n",
    "                'Chi2': chi2,\n",
    "                'p_value': p_value,\n",
    "                'Risk_Ratio': fail_rate_outliers/fail_rate_normal\n",
    "            })\n",
    "\n",
    "if len(chi_square_results) > 0:\n",
    "    chi_df = pd.DataFrame(chi_square_results).sort_values('p_value')\n",
    "    log(f\"Summary: {len(chi_df)} features with significant outlier-failure association\")\n",
    "else:\n",
    "    log(\"No features found with >5x failure rate for outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eebea2",
   "metadata": {},
   "source": [
    "### MULTICOLLINEARITY ANALYSIS (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated feature pairs (potential multicollinearity)\n",
    "log(f\"Searching for highly correlated feature pairs (|r| > 0.85)...\")\n",
    "high_corr_pairs = []\n",
    "perfect_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.85:\n",
    "            if abs(corr_matrix.iloc[i, j]) >= 1.0:  \n",
    "                perfect_corr_pairs.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "            else:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aff709",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Display highly correlated pairs\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\n",
    "    by=['Correlation', 'Feature 1'], ascending=[False, True])\n",
    "perfect_corr_df = pd.DataFrame(perfect_corr_pairs).sort_values(\n",
    "    by=['Correlation', 'Feature 1'], ascending=[False, True])\n",
    "\n",
    "\n",
    "log(f\"\\nHighly correlated feature pairs (|r| > 0.85): {len(high_corr_df)} pairs found\")\n",
    "log(f\"Perfectly correlated feature pairs (|r| >= 1.0): {len(perfect_corr_df)} pairs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b76785",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_vif(X_subset):\n",
    "    \"\"\"Calculate Variance Inflation Factor for each feature.\n",
    "\n",
    "    VIF quantifies multicollinearity:\n",
    "      VIF = 1: No correlation | VIF = 1-5: Moderate | VIF > 5: High | VIF > 10: Severe\n",
    "    \"\"\"\n",
    "    X_filled = X_subset.fillna(0)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"Feature\": X_filled.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_filled.values, i)\n",
    "                for i in range(X_filled.shape[1])]\n",
    "    })\n",
    "    return vif_data.sort_values('VIF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF on all 590 features  (computationally expensive)\n",
    "vif_data = calculate_vif(X)\n",
    "\n",
    "# Check for severe multicollinearity\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "high_vif_features = high_vif['Feature'].tolist()\n",
    "low_vif_features = [f for f in vif_data['Feature'] if f not in high_vif_features]\n",
    "log(f\"{len(high_vif)} features have VIF > 10 (serious multicollinearity)\")\n",
    "log(f\"Features with acceptable VIF (<=10): {len(low_vif_features)}\")\n",
    "log(f\"Worst offenders:\\n{high_vif.head(10).to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f87d47",
   "metadata": {},
   "source": [
    "#### VIF Reduction Strategy: Hierarchical Clustering\n",
    "\n",
    "**Goal:** Reduce multicollinearity by grouping highly correlated features and keeping the best from each cluster\n",
    "\n",
    "**Approach:**\n",
    "1. Cluster high-VIF features by correlation similarity (hierarchical clustering)\n",
    "2. Within each cluster, select feature with highest target correlation\n",
    "3. Remove redundant features from same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f48a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering: impute missing values, remove constant features\n",
    "X_tmp = X[high_vif_features].apply(lambda col: col.fillna(col.median()))\n",
    "non_constant = X_tmp.loc[:, X_tmp.std() > 0]\n",
    "\n",
    "log(f\"Clustering {len(non_constant.columns)} high-VIF features with non-zero variance...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941addc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation-based distance matrix\n",
    "corr = non_constant.corr().abs()\n",
    "corr = corr.fillna(0.0)  # Handle any NaN correlations\n",
    "\n",
    "# Convert correlation to distance: distance = 1 - |correlation|\n",
    "# Features with high correlation have low distance (same cluster)\n",
    "distance_matrix = 1 - corr\n",
    "distance_matrix = distance_matrix.clip(lower=0.0)  # Ensure non-negative\n",
    "\n",
    "# Convert to condensed distance matrix for clustering\n",
    "condensed_distance = squareform(distance_matrix, checks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f20d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering (complete linkage)\n",
    "Z = linkage(condensed_distance, method='complete')\n",
    "\n",
    "# Cut dendrogram at distance threshold 0.10 (correlation >= 0.90)\n",
    "# Features with correlation >= 0.90 are grouped together\n",
    "cluster_labels = fcluster(Z, t=0.10, criterion='distance')\n",
    "\n",
    "clusters = pd.DataFrame({\n",
    "    \"feature\": non_constant.columns,\n",
    "    \"cluster\": cluster_labels\n",
    "}).sort_values(\"cluster\")\n",
    "\n",
    "cluster_sizes = clusters['cluster'].value_counts().sort_values(ascending=False)\n",
    "log(f\"\\nClustering results:\")\n",
    "log(f\"  Total clusters formed: {len(cluster_sizes)}\")\n",
    "log(f\"  Largest cluster size: {cluster_sizes.max()}\")\n",
    "log(f\"  Singleton clusters (size=1): {(cluster_sizes == 1).sum()}\")\n",
    "log(f\"  Multi-feature clusters: {(cluster_sizes > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9081da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best feature from each cluster (highest target correlation)\n",
    "target_corr_tmp = X_tmp.join(y['target']).abs().corr()['target'].drop('target')\n",
    "\n",
    "keepers = (\n",
    "    clusters\n",
    "    .assign(target_corr_tmp=lambda df: df['feature'].map(target_corr_tmp))\n",
    "    .sort_values(['cluster', 'target_corr_tmp'], ascending=[True, False])\n",
    "    .groupby('cluster')\n",
    "    .first()\n",
    ")\n",
    "\n",
    "selected_features = keepers['feature'].tolist()\n",
    "log(f\"\\nFeature selection within clusters:\")\n",
    "log(f\"  Total selected: {len(selected_features)} features (one per cluster)\")\n",
    "log(f\"  Features removed: {len(high_vif_features) - len(selected_features)}\")\n",
    "log(f\"\\nTop 5 selected features by target correlation:\")\n",
    "log(keepers.nlargest(5, 'target_corr_tmp')[['feature', 'target_corr_tmp']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct feature set: keep low-VIF features + selected high-VIF features\n",
    "X_reduced = X.drop(columns=set(high_vif_features) - set(selected_features))\n",
    "log(f\"\\nFinal feature set after VIF reduction:\")\n",
    "log(f\"  Original: {X.shape[1]} features\")\n",
    "log(f\"  Reduced: {X_reduced.shape[1]} features\")\n",
    "log(f\"  Reduction: {X.shape[1] - X_reduced.shape[1]} features removed ({(X.shape[1] - X_reduced.shape[1])/X.shape[1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced\n",
    "calculate_vif(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee0244",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "X_reduced_final = X_reduced.copy()\n",
    "log(f\"Final feature set shape after multicollinearity reduction: {X_reduced_final.shape}\")\n",
    "log(f\"Duplicate columns check: {X_reduced_final.columns.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8962c",
   "metadata": {},
   "source": [
    "## EDA SUMMARY\n",
    "\n",
    "**Dataset Overview:**\n",
    "- 1,567 semiconductor wafer records with 590 sensor features\n",
    "- Binary classification: Pass (0) vs Fail (1)\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Class Imbalance:** Severe 14:1 ratio (Pass:Fail)\n",
    "   - Requires balanced metrics (G-Mean, AUC-ROC) not accuracy\n",
    "   - Needs resampling (SMOTE/ADASYN) or class weighting\n",
    "\n",
    "2. **Missing Data:** 4.5% overall, non-random pattern\n",
    "   - 159 features have >50% missing values\n",
    "   - Missingness correlates with target (MAR/MNAR)\n",
    "\n",
    "3. **Multicollinearity:** Severe (214 features with VIF > 10)\n",
    "   - Hierarchical clustering identifies redundant feature groups\n",
    "   - VIF reduction removes ~67% of features\n",
    "\n",
    "4. **Feature-Target Correlation:** Weak individual correlations\n",
    "   - Max correlation ~0.15 (no single strong predictor)\n",
    "   - Ensemble methods likely needed\n",
    "\n",
    "**Preprocessing Recommendations:**\n",
    "- Remove features with >50-90% missing values\n",
    "- Apply median imputation for remaining missing values\n",
    "- Use LASSO for automatic feature selection\n",
    "- Consider PCA for dimensionality reduction (loses interpretability)\n",
    "\n",
    "**Next Step:** Run preprocessing pipeline\n",
    "```bash\n",
    "make preprocess  # or make pipeline for full run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report with embedded figures\n",
    "def generate_eda_report():\n",
    "    \"\"\"\n",
    "    Generate comprehensive EDA markdown report with all figures.\n",
    "\n",
    "    Creates reports/eda_report.md with:\n",
    "    - Executive summary\n",
    "    - All generated visualizations\n",
    "    - Key findings and recommendations\n",
    "    \"\"\"\n",
    "    report_path = REPORTS_DIR / 'eda_report.md'\n",
    "\n",
    "    report_content = \"\"\"# Exploratory Data Analysis Report\n",
    "## SECOM Semiconductor Defect Prediction\n",
    "\n",
    "**Author:** Victoria A.\n",
    "**Dataset:** UCI SECOM (Semiconductor Manufacturing)\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents the exploratory data analysis of the SECOM semiconductor manufacturing dataset. The analysis reveals critical challenges that inform the modeling strategy:\n",
    "\n",
    "- **Severe class imbalance** (14:1 Pass:Fail ratio) requiring balanced metrics\n",
    "- **Complex missing data patterns** with non-random characteristics\n",
    "- **High multicollinearity** among sensor features requiring dimensionality reduction\n",
    "- **Weak individual feature-target correlations** suggesting ensemble methods\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset Overview\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Samples | 1,567 wafers |\n",
    "| Features | 590 sensor measurements |\n",
    "| Target | Binary (Pass=0, Fail=1) |\n",
    "| Pass Rate | 93.4% |\n",
    "| Fail Rate | 6.6% |\n",
    "| Date Range | ~30 days |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Class Imbalance Analysis\n",
    "\n",
    "The dataset exhibits severe class imbalance with a 14:1 ratio of passing to failing wafers.\n",
    "\n",
    "![Class Imbalance Over Time](figures/class_imbalance_over_time.png)\n",
    "\n",
    "**Key Observations:**\n",
    "- Failure rate varies between 0% and 100% across production days\n",
    "- Some days have very few samples, affecting reliability of daily statistics\n",
    "- Overall imbalance requires G-Mean or AUC-ROC metrics (not accuracy)\n",
    "\n",
    "**Implications for Modeling:**\n",
    "- Cannot use accuracy as primary metric (naive baseline = 93.4%)\n",
    "- Must apply resampling (SMOTE/ADASYN) or class weighting\n",
    "- Threshold optimization critical for balanced sensitivity/specificity\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Missing Data Analysis\n",
    "\n",
    "Overall missing rate: **4.54%** (41,951 missing values across all cells)\n",
    "\n",
    "![Missing Data Matrix](figures/missing_data_matrix.png)\n",
    "\n",
    "![Missing Data Heatmap](figures/missing_data_heatmap.png)\n",
    "\n",
    "**Key Findings:**\n",
    "- 541 features have complete data (no missing values)\n",
    "- 16 features have >60% missing values\n",
    "- Missingness patterns show correlation with target (MAR/MNAR detected)\n",
    "- Groups of features missing together suggest sensor clusters\n",
    "\n",
    "**Preprocessing Strategy:**\n",
    "- Remove features with >50% missing values\n",
    "- Apply KNN imputation (preserves correlational structure)\n",
    "- Consider missingness indicators as engineered features\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature-Target Correlation\n",
    "\n",
    "Individual feature correlations with target are weak, with maximum correlation ~0.15.\n",
    "\n",
    "![Correlation Heatmap](figures/correlation_heatmap_top20.png)\n",
    "\n",
    "**Key Observations:**\n",
    "- No single feature provides strong predictive signal\n",
    "- Top features show mild separation between Pass/Fail distributions\n",
    "- Ensemble methods or feature combinations needed\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Feature Distribution Analysis\n",
    "\n",
    "![Feature Distributions](figures/top_features_distributions.png)\n",
    "\n",
    "![Feature Boxplots](figures/top_features_boxplots.png)\n",
    "\n",
    "**Statistical Tests:**\n",
    "- Mann-Whitney U tests confirm significant distribution differences\n",
    "- Top correlated features show measurable separation\n",
    "- Outliers present in most features (IQR method)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Multicollinearity Analysis\n",
    "\n",
    "Severe multicollinearity detected: **214 features with VIF > 10**\n",
    "\n",
    "**Remediation Strategy:**\n",
    "1. Hierarchical clustering of high-VIF features by correlation\n",
    "2. Select best feature (highest target correlation) from each cluster\n",
    "3. Reduces feature set by ~67% while preserving predictive information\n",
    "\n",
    "**Feature Selection Recommendation:**\n",
    "- Use LASSO regularization for automatic selection\n",
    "- Alternative: PCA for dimensionality reduction (sacrifices interpretability)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Findings Summary\n",
    "\n",
    "| Finding | Impact | Recommendation |\n",
    "|---------|--------|----------------|\n",
    "| 14:1 class imbalance | Accuracy misleading | Use G-Mean, SMOTE/ADASYN |\n",
    "| 4.5% missing data (non-random) | Potential information loss | KNN imputation |\n",
    "| High multicollinearity | Unstable coefficients | LASSO or PCA |\n",
    "| Weak individual correlations | No single strong predictor | Ensemble methods |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Preprocessing Pipeline\n",
    "\n",
    "Based on this EDA, the recommended preprocessing pipeline:\n",
    "\n",
    "```\n",
    "1. Train/Test Split (80/20, stratified) - BEFORE any preprocessing\n",
    "2. Remove features with >50% missing values\n",
    "3. Apply KNN imputation (k=5) on training set\n",
    "4. Transform test set using training parameters\n",
    "5. Standardize features (StandardScaler)\n",
    "6. Apply LASSO for feature selection OR PCA for reduction\n",
    "7. Apply SMOTE/ADASYN on training set only\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Run the preprocessing and modeling pipeline:\n",
    "```bash\n",
    "make pipeline\n",
    "```\n",
    "\n",
    "Or run stages individually:\n",
    "```bash\n",
    "make preprocess  # Data preprocessing\n",
    "make tune        # Hyperparameter tuning\n",
    "make select      # Production model selection\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Report generated from notebooks/eda.py*\n",
    "\"\"\"\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "\n",
    "    log(f\"\\nGenerated EDA report: {report_path}\")\n",
    "    log(f\"  Figures referenced: {len(GENERATED_FIGURES)}\")\n",
    "    for fig_info in GENERATED_FIGURES:\n",
    "        log(f\"    - {fig_info['filename']}: {fig_info['caption']}\")\n",
    "\n",
    "    return report_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc64f25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate the report\n",
    "if __name__ == '__main__' or 'ipykernel' not in sys.modules:\n",
    "    generate_eda_report()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
